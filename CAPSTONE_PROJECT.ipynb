{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAPSTONE_PROJECT",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXDR_PxRs9rK"
      },
      "source": [
        "# LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtQDmvKJt45O"
      },
      "source": [
        "import tarfile\n",
        "\n",
        "#Load data\n",
        "filename = '/content/moral_stories_datasets.tar.xz'\n",
        "\n",
        "moral_stories = tarfile.open(filename)\n",
        "moral_stories.extractall()\n",
        "moral_stories.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiMVc8o90Ng4"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def json_to_dataframe(filename, verbose=False):\n",
        "\n",
        "  with open(filename, 'r') as json_file:\n",
        "    situation = []\n",
        "    action = []\n",
        "    label = []\n",
        "\n",
        "    json_list = list(json_file)\n",
        "    if verbose:\n",
        "      print(len(json_list))\n",
        "\n",
        "    for json_str in json_list:\n",
        "      json_dict = json.loads(json_str)\n",
        "      if verbose:\n",
        "        print(f'json dict: {json_dict}')\n",
        "\n",
        "      if 'situation' in json_dict:\n",
        "        situation.append(json_dict['situation'])\n",
        "\n",
        "      if 'immoral_action' in json_dict:\n",
        "        action.append(json_dict['immoral_action'])\n",
        "\n",
        "      if 'moral_action' in json_dict:\n",
        "        action.append(json_dict['moral_action'])\n",
        "\n",
        "      if 'label' in json_dict:\n",
        "        label.append(json_dict['label'])\n",
        "\n",
        "  df = pd.DataFrame({'situation': situation, 'action': action, 'label': label})\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDUzdIK7e2aw"
      },
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Specify data directory \n",
        "data_dir = os.path.join('moral_stories_datasets', 'classification', 'action+context', 'lexical_bias')\n",
        "\n",
        "#Load training and testing data\n",
        "train_df = json_to_dataframe(os.path.join(data_dir, 'train.jsonl'))\n",
        "test_df = json_to_dataframe(os.path.join(data_dir, 'test.jsonl'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBl1LszG-brb"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pxl1ep7I-ki1"
      },
      "source": [
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd_dWsIQg8hP"
      },
      "source": [
        "#Concatenate text in 'situation' and 'action' columns\n",
        "\n",
        "train_df['situation + action'] = ''\n",
        "test_df['situation + action'] = ''\n",
        "\n",
        "num_train_rows = len(train_df['situation'])\n",
        "num_test_rows = len(test_df['situation'])\n",
        "\n",
        "for i in range(num_train_rows):\n",
        "  action = train_df['action'].iloc[i]\n",
        "  situation = train_df['situation'].iloc[i]\n",
        "  train_df['situation + action'].iloc[i] = f'<start>{situation}<sep>{action}<end>'\n",
        "\n",
        "for j in range(num_test_rows):\n",
        "  action = test_df['action'].iloc[j]\n",
        "  situation = test_df['situation'].iloc[j]\n",
        "  test_df['situation + action'].iloc[j] = f'<start>{situation}<sep>{action}<end>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnSuJnL8ODFk"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhW3o4PaOFQL"
      },
      "source": [
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhOfcKtYhKr3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "7ddb042e-1449-4fcf-ef46-ba9d7bdcd1ef"
      },
      "source": [
        "#Generate dictionary for tokenization\n",
        "\n",
        "import nltk\n",
        "import pickle\n",
        "import argparse\n",
        "from collections import Counter\n",
        "from vist import VIST\n",
        "\n",
        "class Vocabulary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "def build_vocab(textList, threshold):\n",
        "    counter = Counter()\n",
        "    for each_text in textList:\n",
        "        tokens = []\n",
        "        try:\n",
        "            tokens = nltk.tokenize.word_tokenize(each_text.lower())\n",
        "        except Exception:\n",
        "            pass\n",
        "        counter.update(tokens)\n",
        "\n",
        "    if i % 1000 == 0:\n",
        "        print(\"[%d/%d] Tokenized the story captions.\" %(i, len(ids)))\n",
        "\n",
        "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
        "\n",
        "    vocab = Vocabulary()\n",
        "    vocab.add_word('<pad>')\n",
        "    vocab.add_word('<start>')\n",
        "    vocab.add_word('<end>')\n",
        "    vocab.add_word('<unk>')\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        vocab.add_word(word)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def main(args):\n",
        "    # Give your text_list here\n",
        "    vocab = build_vocab(text_list,\n",
        "                        threshold=args.threshold)\n",
        "    vocab_path = args.vocab_path\n",
        "    with open(vocab_path, 'wb') as f:\n",
        "        pickle.dump(vocab, f)\n",
        "    print(\"Total vocabulary size: %d\" %len(vocab))\n",
        "    print(\"Saved the vocabulary wrapper to '%s'\" %vocab_path)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument('--sis_path', type=str,\n",
        "#                         default='./data/sis/train.story-in-sequence.json',\n",
        "#                         help='path for train sis file')\n",
        "    parser.add_argument('--vocab_path', type=str, default='./models/vocab.pkl',\n",
        "                        help='path for saving vocabulary wrapper')\n",
        "    parser.add_argument('--threshold', type=int, default=10,\n",
        "                        help='minimum word count threshold')\n",
        "    args = parser.parse_args()\n",
        "    main(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-61ca3fb76834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVIST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vist'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUOI69zIhNa-"
      },
      "source": [
        "#Tokenize concatenated text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv1v3aEXhSMO"
      },
      "source": [
        "#Generate word embedding\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_BhTHcZtP8O"
      },
      "source": [
        "## EXAMPLES FROM *Text classification with Transformer*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBGGRVErtbap"
      },
      "source": [
        "Implement a Transformer block as a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2xFQYpotWB-"
      },
      "source": [
        "# class TransformerBlock(layers.Layer):\n",
        "#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "#         super(TransformerBlock, self).__init__()\n",
        "#         self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "#         self.ffn = keras.Sequential(\n",
        "#             [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "#         )\n",
        "#         self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "#         self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "#         self.dropout1 = layers.Dropout(rate)\n",
        "#         self.dropout2 = layers.Dropout(rate)\n",
        "        \n",
        "#     def call(self, inputs, training):\n",
        "#         attn_output = self.att(inputs, inputs)\n",
        "#         attn_output = self.dropout1(attn_output, training=training)\n",
        "#         out1 = self.layernorm1(inputs + attn_output)\n",
        "#         ffn_output = self.ffn(out1)\n",
        "#         ffn_output = self.dropout2(ffn_output, training=training)\n",
        "#         return self.layernorm2(out1 + ffn_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HpDxBLKtdFw"
      },
      "source": [
        "Implement embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k1pPF-AthL5"
      },
      "source": [
        "# class TokenAndPositionEmbedding(layers.Layer):\n",
        "#     def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "#         super(TokenAndPositionEmbedding, self).__init__()\n",
        "#         self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "#         self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "#     def call(self, x):\n",
        "#         maxlen = tf.shape(x)[-1]\n",
        "#         positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "#         positions = self.pos_emb(positions)\n",
        "#         x = self.token_emb(x)\n",
        "#         return x + positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEGuubJFtruV"
      },
      "source": [
        "Download and prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd_f4RiVtsEJ"
      },
      "source": [
        "# vocab_size = 20000  # Only consider the top 20k words\n",
        "# maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "# (x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "# print(len(x_train), \"Training sequences\")\n",
        "# print(len(x_val), \"Validation sequences\")\n",
        "# x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "# x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGpstunCtwfn"
      },
      "source": [
        "Create classifier model using transformer layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYk6wX7ltyfs"
      },
      "source": [
        "# embed_dim = 32  # Embedding size for each token\n",
        "# num_heads = 2  # Number of attention heads\n",
        "# ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "# inputs = layers.Input(shape=(maxlen,))\n",
        "# embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "# x = embedding_layer(inputs)\n",
        "# transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "# x = transformer_block(x)\n",
        "# x = layers.GlobalAveragePooling1D()(x)\n",
        "# x = layers.Dropout(0.1)(x)\n",
        "# x = layers.Dense(20, activation=\"relu\")(x)\n",
        "# x = layers.Dropout(0.1)(x)\n",
        "# outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "# model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdlfcWRNt0Ao"
      },
      "source": [
        "Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZeFX7ejt14-"
      },
      "source": [
        "# model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "# history = model.fit(\n",
        "#     x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anvZQdw_bMLk"
      },
      "source": [
        "# #Function to read .json files\n",
        "# def json_file_load(filename, verbose=True):\n",
        "#     #json_data = []\n",
        "#     with open(filename, 'r') as json_file:\n",
        "#         json_list = list(json_file)\n",
        "#         if verbose:\n",
        "#           print(len(json_list))\n",
        "#         for json_str in json_list:\n",
        "#             result = json.loads(json_str)\n",
        "#             #json_data.append(result)\n",
        "#             if verbose:\n",
        "#               print(f\"result: {result}\")\n",
        "#     #return json_data"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}